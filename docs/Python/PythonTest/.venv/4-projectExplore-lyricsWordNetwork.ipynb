{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c511c22-2e31-4348-923a-e542182ddb62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Networking popular words in lyrics\n",
    "For this exercise, we'll create a network of words (based on part of speech or some other classification) that are frequently used in collections of song lyrics assembled by the Olivia Rodriguez project team. For our network example, we ask: Which of the popular words is used the most frequently by particular artists? \n",
    "\n",
    "If you're adapting this to your own project, take some time with your team to think about what's interesting to explore as a network from your project. You can use this cell to sketch out in markdown what you want to do. \n",
    "\n",
    "The steps to create this network are:\n",
    "## Collect the words, rank them, and select them\n",
    "* Collect the distinct words from all of the lyrics together by part of speech. (Let's look at nouns in this example.) Return these as a sorted list with duplicates removed, ranked from most to least frequent. \n",
    "* Streamline the list, by choosing, say, the top 10 or 20 words.\n",
    "\n",
    "## Find out which artists use each word and how much\n",
    "* Reach into the collections assembled by artist.\n",
    "* For each word in our streamlined list, return a count of how much the artist repeats that word\n",
    "* Prepare network data (arranged as a TSV or pandas dataframe) with this structure. (The syntax will be different, but this is just an idea of the information we need.\n",
    "\n",
    "  ```\n",
    "     word | used by | artist | count of times used by this artist\n",
    "  ```\n",
    "\n",
    "### Alternative ways to develop networks of information\n",
    "There are lots of ways to think about how to explore word use in song lyrics. We are making something of a \"big picture\" study of the most popular words by part of speech used by all artists, and looking across their albums: a word is just used by an artist (regardless of which song or album it's in). But we could change this to take a closer look at other patterns. For example:\n",
    "* Change the context: word use by song or within an album!\n",
    "\n",
    "* Start with a word of interest **to accept as input into the notebook** and:\n",
    "    * Look for all the ones most closely related in a collection using cosine similarity (see our early Python homework assignments to explore words similar to a word of interest).\n",
    "    * Try an adjacency network of words: Find out which other words of its type are sitting close (adjacent) to the word of interest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656b4f0-3148-4a0d-84ba-32ec670d56c9",
   "metadata": {},
   "source": [
    "# Practical considerations: an overview of our process!\n",
    "## Consider how to visualize this network\n",
    "This would be a bimodal network to show which words are shared the most by which artists in the collection. \n",
    "Node1 = word\n",
    "Node2 = artist\n",
    "Edge connection = \"used by\"\n",
    "Edge weight data = count of times used by this artist (needs to be an integer)\n",
    "\n",
    "## How will we do all this (with the Olivia Rodriguez Team collections)?\n",
    "* Python imports and functions to coordinate processing\n",
    "* Apply saxonche to import the team's XML: pull the text you want from the XML nodes in a collection using XQuery\n",
    "    * _Without XML data_, collect strings of words by opening the files and reading them in. Refer to our early Python assignments.\n",
    "* Use NLP to find the words of interest: We can send the text for the whole collection to spaCy (much as we did in [project ipynb exercise 3](3-projectExplore-dataCounts.ipynb) to retrieve the words of interest (e.g. nouns).\n",
    "    * Remove duplicates and rank them (use Counter and mostCommon()).\n",
    "    * Slice this list to get you the top 20 (or however many you want to plot).\n",
    "      \n",
    "* Now return to our XML collection to find out information about who is using the words and where.\n",
    "    * For our example from the Olivia Rodriguez team, the team has organized files in folders named by artist and album. We'll use this organization to help collect information based on the artists. (See local folder in this projectExamples directory named `lyricXML/`.\n",
    "    * We'll return to saxonche and **write an XQuery function that defines the collections we need to reach into based on the artist**. (**NOTE: This part is tricky: It will be specific to the project team's folder structure**. Ask for help if you need to on this part to adapt to your project!)\n",
    "        * _Without XML data_, return to the text files: open them based on filename or folder name or whatever structure will help you establish context for your files\n",
    "    * Start with a for loop over your words of interest. Each word is sent into XQuery to retrieve how much it's used by an artist and return the count of its use.\n",
    "        *  _Without XML data_, each word is sent by a Python function retrieve its count of how much it's used in whatever you're using to delimit a special \"bucket\" of files or folders in your project.\n",
    "     \n",
    "    * Output a pandas dataframe  to prepare data to be read by NetworkX and pyVis.\n",
    " \n",
    "## Time to code this..."
   ]
  },
  {
   "cell_type": "code",
   "id": "7abbbcbb-22fc-4d5b-9556-c496d4d9ba6f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# START WITH INSTALLS AND IMPORTS!\n",
    "\n",
    "# If you're missing anything in the import cells below, you should install it with pip (or pip3) in your virtual environment. \n",
    "\n",
    "# TRY uncommenting the lines here to see if the notebook will handle the imports directly. \n",
    "# Here you need the `!` in front if it's going to work.\n",
    "# IF THAT DOESN'T WORK:\n",
    "# (Go to your command line in the git bash shell (Windows) or Terminal (Mac) and \n",
    "# activate your virtual environment where you've set it on your local computer: \n",
    "# Windows: source Scripts/activate\n",
    "# Mac: source bin/activate \n",
    "# watch for your virtual environment to show you it's active in the shell.\n",
    "# Then enter your pip installs (without the `!` explanation points). \n",
    "\n",
    "# INSTALLS\n",
    "# !pip install pathlib\n",
    "# !pip install saxonche\n",
    "# !pip install pandas\n",
    "# !pip install networkx\n",
    "# !pip install pyvis\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f3df611-6f93-4aed-907c-fbc531c18830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T22:13:10.237763Z",
     "start_time": "2024-05-03T22:13:09.504124Z"
    }
   },
   "source": [
    "# IMPORTS for the text NLP processing\n",
    "import pathlib\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from saxonche import PySaxonProcessor\n",
    "from collections import Counter\n",
    "\n",
    "# Just in case you want it:\n",
    "\n",
    "# import re as regex\n",
    "# re is standard to Python3: lets us work with regular expressions in Python. \n",
    "# Uncomment it if you want to try it here to search for a specific pattern in your texts with Python."
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7b8bee6a-7944-4d26-8804-6ff459ca37d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T22:17:47.575002Z",
     "start_time": "2024-05-03T22:17:47.564012Z"
    }
   },
   "source": [
    "# IMPORTS For the network visualizations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetworkx\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnx\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyvis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnetwork\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Network\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ec561-69cb-4fbe-b6ec-bb8c25b9912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.cli.download(\"en_core_web_lg\")\n",
    "# ONLY NEED ABOVE LINE ONCE. REMEMBER: COMMENT OUT THE ABOVE LINE THE NEXT TIME YOU RUN THIS.\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5d647-24ba-4ba0-8aae-2a0bdf08a475",
   "metadata": {},
   "source": [
    "## Collect the words, rank them, and select them\n",
    "* Collect the distinct words from all of the lyrics together by part of speech. (Let's look at nouns in this example.) Return these as a sorted list with duplicates removed, ranked from most to least frequent. \n",
    "* Streamline the list, by choosing, say, the top 10 or 20 words.\n",
    "\n",
    "\n",
    "### Sample XML code for a file in the lyricXML collection\n",
    "\n",
    "```xml\n",
    "<lyrics>\n",
    "    <section type=\"verse\" n=\"1\">\n",
    "        <l>Told you not to worry</l>\n",
    "        <l>But maybe that's a lie</l>\n",
    "        <l>Honey, what's your hurry?</l>\n",
    "        <l>Won't you stay inside?</l>\n",
    "        <l>Remember not to get too close to stars</l>\n",
    "        <l>They're never gonna give you love like ours</l>\n",
    "    </section>\n",
    "    <section type=\"chorus\">\n",
    "        <l>Where did you go?</l>\n",
    "        <l>I should know, but it's cold</l>\n",
    "        <l>And I don't wanna be lonely</l>\n",
    "        <l>So show me the way home</l>\n",
    "        <l>I can't lose another life</l>\n",
    "    </section>\n",
    "    <section type=\"refrain\">\n",
    "        <l>Hurry, I'm worried</l>\n",
    "    </section>\n",
    "    <section type=\"verse\" n=\"2\">\n",
    "        <l>The world's a little blurry</l>\n",
    "        <l>Or maybe it's my eyes</l>\n",
    "        <l>The friends I've had to bury</l>\n",
    "        <l>They keep me up at night</l>\n",
    "        <l>Said I couldn't love someone</l>\n",
    "        <l>'Cause I might break</l>\n",
    "        <l>If you're gonna die, not by mistake</l>\n",
    "    </section>\n",
    "    <section type=\"chorus\">\n",
    "        <l>So, where did you go?</l>\n",
    "        <l>I should know, but it's cold</l>\n",
    "        <l>And I don't wanna be lonely</l>\n",
    "        <l>So tell me you'll come home</l>\n",
    "        <l>Even if it's just a lie</l>\n",
    "    </section>\n",
    "    <section type=\"bridge\">\n",
    "        <l>I tried not to upset you</l>\n",
    "        <l>Let you rescue me the day I met you</l>\n",
    "        <l>I just wanted to protect you</l>\n",
    "        <l>But now I'll never get to</l>\n",
    "    </section>\n",
    "    <section type=\"refrain\">\n",
    "        <l>Hurry, I'm worried</l>\n",
    "    </section>\n",
    "    <section type=\"outro\">\n",
    "        <l>Where did you go?</l>\n",
    "        <l>I should know, but it's cold</l>\n",
    "        <l>And I don't wanna be lonely</l>\n",
    "        <l>Was hoping you'd come home</l>\n",
    "        <l>I don't care if it's a lie</l>\n",
    "    </section>\n",
    "</lyrics>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbeffe-ba84-444b-a2d0-2ab1cd4f313d",
   "metadata": {},
   "source": [
    "### The next two cells...\n",
    "Define your input and output filepaths...and send them to XQuery for processing.\n",
    "\n",
    "#### About reaching into your file collections with XQuery\n",
    "Our input for this exericse in lyricXML is a set of nested folders, so we need to recurse through them. \n",
    "\n",
    "The collection() function in our XQuery is set to **recurse** through each of the folders   and find all the XML files inside. \n",
    "\n",
    "#### Keeping your outputs from scrolling forever\n",
    "On an output cell with a LONG BLOB of text, right-click and select \"Enable Scrolling for Outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79290d66-bd8a-450b-b1ec-0a8a58aeaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SOME FILE PATHS FOR INPUT, AND (ONCE WE'RE READY) OUTPUT\n",
    "InputPath = 'lyricXML'\n",
    "OutputPath = 'testOutput' \n",
    "\n",
    "# NOTE: We need to use a return line on this function to return the string value of `r` as the result of our python function.\n",
    "# With the return line, that makes it possible to call the function in the next cell when we need to deliver the output to nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123864e-bfbe-44a0-a43c-1a24d776836c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xqueryAndNLP(InputPath):\n",
    "    # This time, let's try XQuery over a collection of files:\n",
    "    with PySaxonProcessor(license=False) as proc:\n",
    "        print(proc.version)\n",
    "        xq = proc.new_xquery_processor()\n",
    "        xq.set_query_base_uri(Path('.').absolute().as_uri() + '/')\n",
    "        xq.set_query_content('''\n",
    "let $allTheLyrics := collection('lyricXML/.?select=*.xml;recurse=yes')\n",
    "(: ebb: our collection variable is set to recurse through the internal nested folders. :)\n",
    "let $lines := $allTheLyrics//l ! text()\n",
    "return string-join($lines, ' ')\n",
    "\n",
    "''')\n",
    "        r = xq.run_query_to_string()\n",
    "        # print(r)\n",
    "        r = str(r)\n",
    "    return r\n",
    "\n",
    "xqueryAndNLP(InputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83276280-241e-4c0d-8a65-edb02b0d75aa",
   "metadata": {},
   "source": [
    "### Let's roll this ball of text over to NLP now. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0337f-344d-409c-b7b1-7adfa7d4185b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If everything's working properly and you have lots of text for the computer to read, this cell may take a moment to run. \n",
    "\n",
    "inputstring = xqueryAndNLP(InputPath)\n",
    "\n",
    "# start playing with spaCy and nlp:\n",
    "words = nlp(inputstring)\n",
    "# print(words)\n",
    "\n",
    "# Collecting the lemmatized forms will be better than just all the words. (Remember what these are?)\n",
    "Lemmas = []\n",
    "for token in words:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        lemma = token.lemma_\n",
    "        Lemmas.append(lemma)\n",
    "\n",
    "# Okay, we'll use python's Counter() find out how frequently each verb lemma shows up in the entire verb list.\n",
    "# Counter() removes duplicates and counts the number of times something appears. \n",
    "# And it outputs a dictionary of key:value pairs already sorted from highest to lowet count.\n",
    "\n",
    "# print(Lemmas)\n",
    "\n",
    "lemmaFreq = Counter(Lemmas)\n",
    "totalLemmaCount = len(lemmaFreq) \n",
    "\n",
    "print(f\"Lemma count: {totalLemmaCount}\")\n",
    "\n",
    "print(f\"Lemma frequency {lemmaFreq}\")\n",
    "\n",
    "# We can even calculate the percentage each verb is used.\n",
    "# The totalVerbCount will be the length of the BenderLemmas list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45e664-8e4e-4734-95e1-14190ab8a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with our previous bar graph examples in exercise 3, we don't want to plot every last word here.\n",
    "# But we have a lot of data, so we can experiment!\n",
    "# To access data in our Counter list and keep it organized from highest to lowest value, we use `most_common()`.\n",
    "# Then we can slice it to store however many we want to plot. [:10] would plot the first 11 values since python starts counting from zero.\n",
    "\n",
    "mostCommon = dict(lemmaFreq.most_common()[:29])\n",
    "print(f\"mostCommon Lemmas {mostCommon}\")\n",
    "\n",
    "# Here we are unpacking our sliced dictionary of most common noun lemmas into lists of the values and keys,\n",
    "# and checking to make sure they remain in their dictionary order here. \n",
    "# We will use the list of lemmas in the next code cell to look for each one as used by each artist.  \n",
    "# (We used them when plotting bar graphs, \n",
    "# so you could output some bar graphs in the next cells if you want, and then return to the network we're building!\n",
    "\n",
    "listCounts = list(mostCommon.values())\n",
    "listLems = list(mostCommon.keys())\n",
    "print(f\"listCounts: {listCounts}\")\n",
    "print(f\"listLems: {listLems}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3af5dd-0af6-4349-ade3-16a889a3046f",
   "metadata": {},
   "source": [
    "## Okay, time to build our network...\n",
    "### Find out which artists use each word and how much\n",
    "* Reach into the collections assembled by artist, **with XQuery again, but this time, reaching into specific collections for each artist** \n",
    "* **For each word in our streamlined list**, return a count of how much the artist repeats that word\n",
    "* Prepare network data (arranged as a TSV or pandas dataframe) with this structure.\n",
    "* This is just to remind us what we're  constructing for our network. **The syntax for our dataframes will be different** (no vertical `|`'s ). \n",
    "\n",
    "  ```\n",
    "     word | used by | artist | count of times used by this artist\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb1b9c-c6b8-4a44-b4ee-96e657686c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def networkClass(listLems, InputPath):\n",
    "    with PySaxonProcessor(license=False) as proc:\n",
    "       for lemma in listLems:\n",
    "            xq = proc.new_xquery_processor()\n",
    "            xq.set_query_base_uri(Path('.').absolute().as_uri() + '/')\n",
    "            xquery = f'''\n",
    "                declare variable $lemma as xs:string* external := '{lemma}';\n",
    "                let $allTheLyrics := collection('lyricXML/.?select=*.xml;recurse=yes')\n",
    "                (: ebb: our collection variables are set to recurse through the internal nested folders.:)\n",
    "                \n",
    "                let $artistNames := ('billie', 'olivia', 'sabrina', 'taylor')\n",
    "                for $name in $artistNames\n",
    "                return $name\n",
    "                \n",
    "            '''\n",
    "            xq.set_query_content(xquery)\n",
    "        \n",
    "    \n",
    "            r = xq.run_query_to_value()\n",
    "            r = str(r)\n",
    "            print(r)\n",
    "\n",
    "    return r\n",
    "    \n",
    "networkClass(listLems, InputPath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1403e09-bac5-4147-9b75-2d292f8ad3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "OutputPath = 'testOutput/networkdata.tsv' \n",
    "\n",
    "def networkQuery(listLems, InputPath):\n",
    "    listdfs = []\n",
    "    with PySaxonProcessor(license=False) as proc:\n",
    "       for lemma in listLems:\n",
    "            xq = proc.new_xquery_processor()\n",
    "            xq.set_query_base_uri(Path('.').absolute().as_uri() + '/')\n",
    "            xquery = f'''\n",
    "                declare variable $lemma as xs:string* external := '{lemma}';\n",
    "                declare variable $string as xs:string := string-join(\n",
    "                let $billieLyrics := collection('lyricXML/billie/.?select=*.xml;recurse=yes')\n",
    "                let $allTheLyrics := collection('lyricXML/.?select=*.xml;recurse=yes')\n",
    "                (: ebb: our collection variables are set to recurse through the internal nested folders.:)\n",
    "                \n",
    "                let $artistNames := ('billie', 'olivia', 'sabrina', 'taylor')\n",
    "                    for $name in $artistNames\n",
    "                    (: return $name :) \n",
    "                \n",
    "                    let $lemmaLines := $allTheLyrics[base-uri() ! contains(., $name)]//l ! text()[contains(., $lemma)]\n",
    "                    let $billieCount := count($lemmaLines)\n",
    "                    return ($lemma || '\\t' || 'used by' || '\\t' || $name || '\\t' ||  $billieCount), '\\n');\n",
    "                \n",
    "                (: May work more reliably than regex '\\n' :)\n",
    "                (: IF NEEDED: in place of \\t, we can use `&#x9;.` :)\n",
    "                (: IF NEEDED: in pace of \\n, we can use this weird special character for a newline or hard return.:)\n",
    "                $string\n",
    "            '''\n",
    "            xq.set_query_content(xquery)\n",
    "        \n",
    "            r = xq.run_query_to_value()\n",
    "            r = str(r)\n",
    "            # print(r)\n",
    "            # ebb: Now we read this into a pandas dataframe based on tab-separated values as csv/tsv data:\n",
    "            df = pd.read_csv(StringIO(r), header=None, sep=\"\\t\")\n",
    "            # print(df) # ebb: This churns out lots of little dataframes based on each turn of the python for loop in this function.\n",
    "            # So, we need to bundle them together. We'll start by putting them in a list. \n",
    "            listdfs.append(df)\n",
    "            #print(listdfs)\n",
    "    # ebb: Now we concatenate the list of pandas dataframes into just one using pd.concat:\n",
    "    merged_df = pd.concat(listdfs, ignore_index=True)\n",
    "    merged_df.to_csv(OutputPath, sep=\"\\t\") \n",
    "           \n",
    "\n",
    "    return(merged_df)\n",
    "    \n",
    "networkQuery(listLems, InputPath)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cfa3a-e0fa-4d3b-925e-654a4086cedf",
   "metadata": {},
   "source": [
    "### Network Vis Time!\n",
    "We have lovely network data formatted as pandas dataframes. (We could have output the data and bundled it up in a dictinoary structure or something else, but we thought we'd try dataframes for ease of reading. \n",
    "\n",
    "Dataframes are used frequently in text data analytics for organizing and reading values into charts and graphs. Read more about it at <https://www.geeksforgeeks.org/python-pandas-dataframe/>.\n",
    "\n",
    "Now we need to send the dataframes to networkx and pyvis for networking. \n",
    "\n",
    "#### Fine-tuning the network display\n",
    "Here are some resources for adjusting how the network displays. \n",
    "* PyVis Network documentation: <https://pyvis.readthedocs.io/en/latest/documentation.html>\n",
    "* Using the Configuration UI to tweak the Network: <https://pyvis.readthedocs.io/en/latest/tutorial.html#using-the-configuration-ui-to-dynamically-tweak-network-settings>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa6f72-1ef3-41bb-93ba-5110a8136193",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkData = networkQuery(listLems, InputPath)   \n",
    "# ebb: networkData variable is storing the dataframes output of the previous cell\n",
    "\n",
    "# Create the network graph\n",
    "net = Network(height='600px', width='100%', bgcolor='#222222', font_color='black', notebook=True, select_menu=True, cdn_resources=\"in_line\")\n",
    "\n",
    "\n",
    "# Iterate through the DataFrame and add nodes and edges\n",
    "for i, row in networkData.iterrows():\n",
    "    source = row[0]\n",
    "    target = row[2]\n",
    "    weight = row[3]\n",
    "    net.add_node(source, shape='circle')\n",
    "    net.add_node(target, shape='box', color='pink', size=2500)\n",
    "    net.add_edge(source, target, value=weight*10, title=weight)\n",
    "\n",
    "\n",
    "# Customize the layout\n",
    "# ebb: see PyVis docs: https://pyvis.readthedocs.io/en/latest/documentation.html#pyvis.network.Network.barnes_hut\n",
    "# I'm trying out / commenting out various combinations of network properties here:\n",
    "# net.barnes_hut()\n",
    "net.barnes_hut(gravity=-80000, central_gravity=0.003, spring_length=5, spring_strength=.1, damping=0.09, overlap=0)\n",
    "\n",
    "# print(net)\n",
    "# Display the graph in the Jupyter Notebook\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show('network_graph.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fda57a-b042-4797-b526-c4bc3df41c87",
   "metadata": {},
   "source": [
    "### Applying network statistics to the visualization\n",
    "#### For experimentation, discussion, project development\n",
    "\n",
    "\n",
    "Our network visualization should be already displaying weighted edges based on the count value (the number of times an artist uses a word). \n",
    "\n",
    "Nodes are not sized by network stats yet. \n",
    "What about applying color and size and other visual properties based on network statistics?\n",
    "\n",
    "For this we need to work with NetworkX and PyVis libraries together. NetworkX calculates network statistics to apply to our visual plot. We'll have to redo our graph to generate network centrality calculations.\n",
    "\n",
    "\n",
    "Network properties to investigate:\n",
    "* degree centrality\n",
    "* closeness centrality\n",
    "* eigenvector centrality\n",
    "* eccentricity\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae7ebf-da82-427d-a6b8-bfc7de7445d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ebb: UNDER CONSTRUCTION! \n",
    "networkData = networkQuery(listLems, InputPath)   \n",
    "\n",
    "# Create the network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "# Iterate through the DataFrame and add nodes and edges\n",
    "for i, row in networkData.iterrows():\n",
    "    source = row[0]\n",
    "    target = row[2]\n",
    "    weight = row[3]\n",
    "    G.add_node(source, shape='dot')\n",
    "    G.add_node(target, shape='triangle')\n",
    "    if target == \"billie\":\n",
    "        colorEdge = \"blue\"\n",
    "    elif target == \"olivia\": \n",
    "        colorEdge = \"green\"\n",
    "    # elif target == \"sabrina\":\n",
    "    #    colorEdge = \"coral\"\n",
    "    # elif target == \"taylor\":\n",
    "    #    colorEdge = \"purple\"\n",
    "    else:\n",
    "        colorEdge = \"coral\"\n",
    "    # How to write Python if else conditions: https://www.w3schools.com/python/python_conditions.asp \n",
    "    G.add_edge(source, target, value=weight, title=weight, color=colorEdge)\n",
    "\n",
    "# Calculate this network's centrality statistics\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "# print(degree_centrality)\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "eccentricity = nx.eccentricity(G)\n",
    "print(eccentricity)\n",
    "\n",
    "# VISUALIZE THE NETWORKX NETWORK IN PYVIS\n",
    "\n",
    "# Create node size list based on closeness centrality\n",
    "node_sizes = [v * 20 for v in degree_centrality.values()]\n",
    "print(node_sizes)\n",
    "\n",
    "# Generate node colors based on degree or eigenvector centrality\n",
    "node_colorVals = [c * 20 for c in closeness_centrality.values()]\n",
    "\n",
    "# Create PyVis Network object\n",
    "net = Network(height='600px', width='100%', bgcolor='#222222', font_color='white', notebook=True, select_menu=True, cdn_resources=\"in_line\")\n",
    "\n",
    "\n",
    "# Add nodes and edges to PyVis Network\n",
    "for node in G.nodes:\n",
    "    # print(node, '||', node_sizes[list(G.nodes).index(node)])\n",
    "    cv = node_colorVals[list(G.nodes).index(node)] * 50\n",
    "    # ebb: Here we'll try basing the COLOR and SIZE of the nodes based on network calculations. \n",
    "    # Follow the code to see which variables store the network information. \n",
    "    # We set rgba Red, Green, Blue color values: https://www.w3schools.com/cssref/func_rgba.php\n",
    "    net.add_node(node, shape=G.nodes[node]['shape'], color=f\"rgba({cv}, 255, 255, 0.8)\", size=node_sizes[list(G.nodes).index(node)])\n",
    "# print(list(G.nodes).index(node))\n",
    "\n",
    "for source, target, edge_data in G.edges(data=True):\n",
    "    print(edge_data)\n",
    "    net.add_edge(source, target, value=edge_data['value'], color=edge_data['color'], title=edge_data['title'])\n",
    "# Show the interactive plot\n",
    "\n",
    "# print(net)\n",
    "# net.barnes_hut(gravity=80, central_gravity=0.0005, spring_length=50, spring_strength=.1, damping=0.09, overlap=0)\n",
    "# net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0)[source]\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show('degreeNetworkVis.html')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ede23-0aea-4fec-8483-ac8bf1d656f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b4240-fffd-4cab-9874-ed33f70607ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
